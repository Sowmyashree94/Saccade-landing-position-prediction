{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Dropout,Flatten,Concatenate, Masking\n",
    "from keras.layers import LSTM\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If GPU is not available: \n",
    "# GPU_USE = '/cpu:0'\n",
    "# config = tf.ConfigProto(device_count = {\"GPU\": 0})\n",
    "\n",
    "\n",
    "# If GPU is available: \n",
    "config = tf.ConfigProto()\n",
    "config.log_device_placement = True\n",
    "config.allow_soft_placement = True\n",
    "config.gpu_options.allocator_type = 'BFC'\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "# Limit the maximum memory used\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "\n",
    "# set session config\n",
    "tf.keras.backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_multiDim_loss(y_true, y_pred):\n",
    "    return K.mean(K.sum(K.square(y_true - y_pred),axis = -1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 16, 2)\n"
     ]
    }
   ],
   "source": [
    "#input data\n",
    "import os\n",
    "data_dir = \"/home/niteesh/Documents/uni/HCI/Saarland/Npy_files/\"\n",
    "files = os.listdir(data_dir)\n",
    "for f in files:\n",
    "    file = data_dir + f\n",
    "    if f== files[0]:\n",
    "        data = np.load(file)\n",
    "    else:\n",
    "        data = np.vstack((data,np.load(file)))\n",
    "        \n",
    "data = np.load(data_dir + \"7.npy\")\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper-parameters, constants\n",
    "max_length_saccades = 7\n",
    "learning_rate=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 7, 2)\n",
      "(720, 2)\n"
     ]
    }
   ],
   "source": [
    "#format data\n",
    "Input = data[:,0:max_length_saccades,:]\n",
    "y = data[:,-1,:]\n",
    "print(Input.shape)\n",
    "print(y.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(Input, y, test_size=0.33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mask_layer (Masking)         (None, 7, 2)              0         \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               (None, 7, 16)             1216      \n",
      "_________________________________________________________________\n",
      "lstm_23 (LSTM)               (None, 16)                2112      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "output_layers (Dense)        (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 3,634\n",
      "Trainable params: 3,634\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Inputs: (None, 7, 2)\n",
      "Outputs: (None, 2)\n",
      "Actual input: (720, 7, 2)\n",
      "Actual output: (720, 2)\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=learning_rate)\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "layer1 = model.add(Masking(mask_value=np.inf, input_shape=(max_length_saccades, 2),name = 'mask_layer'))\n",
    "\n",
    "layer2 = model.add(LSTM(16, return_sequences=True,))\n",
    "\n",
    "layer3 = model.add(LSTM(16,return_sequences=False,))  \n",
    "# returns a sequence of vectors of dimension 32\n",
    "\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(LSTM(8, return_sequences=True))  \n",
    "# returns a sequence of vectors of dimension 32\n",
    "\n",
    "\n",
    "#layer4 = model.add(Dropout(0.2))\n",
    "#model.add(Flatten())\n",
    "\n",
    "model.add(Dense(16, activation='relu'))\n",
    "\n",
    "output_layer = model.add(Dense(2, activation='linear',name = 'output_layers'))\n",
    "#output_layer = model.add(LSTM(2,activation=None,return_sequences=False)) \n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "#model.compile(loss='mse',optimizer=optimizer,metrics=['mse'])\n",
    "\n",
    "model.compile(loss=mse_multiDim_loss,\n",
    "              optimizer=optimizer,)\n",
    "# Print summary\n",
    "model.summary()\n",
    "print(\"Inputs: {}\".format(model.input_shape))\n",
    "print(\"Outputs: {}\".format(model.output_shape))\n",
    "print(\"Actual input: {}\".format(Input.shape))\n",
    "print(\"Actual output: {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 648 samples, validate on 72 samples\n",
      "Epoch 1/200\n",
      "648/648 [==============================] - 2s 3ms/step - loss: 2997977.5586 - val_loss: 3471931.7500\n",
      "Epoch 2/200\n",
      "648/648 [==============================] - 0s 662us/step - loss: 2840330.1728 - val_loss: 3295870.3056\n",
      "Epoch 3/200\n",
      "648/648 [==============================] - 0s 643us/step - loss: 2654605.5309 - val_loss: 3085845.8611\n",
      "Epoch 4/200\n",
      "648/648 [==============================] - 0s 647us/step - loss: 2437864.5123 - val_loss: 2846889.1806\n",
      "Epoch 5/200\n",
      "648/648 [==============================] - 0s 653us/step - loss: 2201989.4259 - val_loss: 2591629.1597\n",
      "Epoch 6/200\n",
      "648/648 [==============================] - 0s 647us/step - loss: 1955232.5216 - val_loss: 2332334.7847\n",
      "Epoch 7/200\n",
      "648/648 [==============================] - 0s 641us/step - loss: 1711457.5093 - val_loss: 2084913.8750\n",
      "Epoch 8/200\n",
      "648/648 [==============================] - 0s 670us/step - loss: 1488156.3410 - val_loss: 1857044.0278\n",
      "Epoch 9/200\n",
      "648/648 [==============================] - 0s 683us/step - loss: 1295213.5231 - val_loss: 1665418.3924\n",
      "Epoch 10/200\n",
      "648/648 [==============================] - 0s 719us/step - loss: 1142190.9491 - val_loss: 1509432.2049\n",
      "Epoch 11/200\n",
      "648/648 [==============================] - 0s 643us/step - loss: 1022175.0224 - val_loss: 1372296.0868\n",
      "Epoch 12/200\n",
      "648/648 [==============================] - 0s 667us/step - loss: 940673.3858 - val_loss: 1271624.0972\n",
      "Epoch 13/200\n",
      "648/648 [==============================] - 0s 666us/step - loss: 885967.9398 - val_loss: 1196346.5694\n",
      "Epoch 14/200\n",
      "648/648 [==============================] - 0s 658us/step - loss: 864242.5231 - val_loss: 1137218.1528\n",
      "Epoch 15/200\n",
      "648/648 [==============================] - 0s 667us/step - loss: 850078.4491 - val_loss: 1223843.3194\n",
      "Epoch 16/200\n",
      "648/648 [==============================] - 1s 779us/step - loss: 836097.2315 - val_loss: 1092493.5972\n",
      "Epoch 17/200\n",
      "648/648 [==============================] - 0s 671us/step - loss: 822584.7593 - val_loss: 1087426.8750\n",
      "Epoch 18/200\n",
      "648/648 [==============================] - 0s 659us/step - loss: 833314.8634 - val_loss: 1083974.0208\n",
      "Epoch 19/200\n",
      "648/648 [==============================] - 0s 675us/step - loss: 837232.3302 - val_loss: 1098268.4583\n",
      "Epoch 20/200\n",
      "648/648 [==============================] - 0s 669us/step - loss: 831389.5401 - val_loss: 1049562.4861\n",
      "Epoch 21/200\n",
      "648/648 [==============================] - 0s 719us/step - loss: 825570.6605 - val_loss: 1053665.0556\n",
      "Epoch 22/200\n",
      "648/648 [==============================] - 0s 656us/step - loss: 823322.1173 - val_loss: 1059145.1944\n",
      "Epoch 23/200\n",
      "648/648 [==============================] - 0s 679us/step - loss: 817612.3225 - val_loss: 999925.0903\n",
      "Epoch 24/200\n",
      "648/648 [==============================] - 0s 674us/step - loss: 817185.7168 - val_loss: 989288.9792\n",
      "Epoch 25/200\n",
      "648/648 [==============================] - 0s 751us/step - loss: 814626.2793 - val_loss: 977506.1736\n",
      "Epoch 26/200\n",
      "648/648 [==============================] - 0s 720us/step - loss: 813160.3634 - val_loss: 968924.1806\n",
      "Epoch 27/200\n",
      "648/648 [==============================] - 0s 715us/step - loss: 835928.2145 - val_loss: 981033.6875\n",
      "Epoch 28/200\n",
      "648/648 [==============================] - 0s 677us/step - loss: 808481.7840 - val_loss: 1027085.9167\n",
      "Epoch 29/200\n",
      "648/648 [==============================] - 0s 670us/step - loss: 821784.1250 - val_loss: 987190.2222\n",
      "Epoch 30/200\n",
      "648/648 [==============================] - 0s 670us/step - loss: 821939.3086 - val_loss: 981690.8333\n",
      "Epoch 31/200\n",
      "648/648 [==============================] - 0s 656us/step - loss: 814905.0066 - val_loss: 968891.4653\n",
      "Epoch 32/200\n",
      "648/648 [==============================] - 0s 739us/step - loss: 799942.7106 - val_loss: 963245.4167\n",
      "Epoch 33/200\n",
      "648/648 [==============================] - 0s 686us/step - loss: 821414.5679 - val_loss: 979400.4861\n",
      "Epoch 34/200\n",
      "648/648 [==============================] - 0s 661us/step - loss: 809658.2716 - val_loss: 960445.8056\n",
      "Epoch 35/200\n",
      "648/648 [==============================] - 0s 674us/step - loss: 815885.3673 - val_loss: 984118.2500\n",
      "Epoch 36/200\n",
      "648/648 [==============================] - 0s 736us/step - loss: 818315.9213 - val_loss: 1007139.1250\n",
      "Epoch 37/200\n",
      "648/648 [==============================] - 0s 708us/step - loss: 817880.2739 - val_loss: 1018192.0694\n",
      "Epoch 38/200\n",
      "648/648 [==============================] - 0s 666us/step - loss: 809499.2207 - val_loss: 1028587.9861\n",
      "Epoch 39/200\n",
      "648/648 [==============================] - 0s 683us/step - loss: 813282.2685 - val_loss: 1005833.1944\n",
      "Epoch 40/200\n",
      "648/648 [==============================] - 0s 668us/step - loss: 808435.6142 - val_loss: 1002338.2222\n",
      "Epoch 41/200\n",
      "648/648 [==============================] - 0s 655us/step - loss: 799170.1250 - val_loss: 992098.4722\n",
      "Epoch 42/200\n",
      "648/648 [==============================] - 0s 670us/step - loss: 804871.5918 - val_loss: 970634.7361\n",
      "Epoch 43/200\n",
      "648/648 [==============================] - 0s 739us/step - loss: 794357.0494 - val_loss: 966867.7153\n",
      "Epoch 44/200\n",
      "648/648 [==============================] - 0s 672us/step - loss: 792618.7971 - val_loss: 978731.8889\n",
      "Epoch 45/200\n",
      "648/648 [==============================] - 0s 766us/step - loss: 814282.6019 - val_loss: 988853.8611\n",
      "Epoch 46/200\n",
      "648/648 [==============================] - 0s 646us/step - loss: 797722.6080 - val_loss: 946009.6250\n",
      "Epoch 47/200\n",
      "648/648 [==============================] - 0s 679us/step - loss: 796769.2284 - val_loss: 981897.3611\n",
      "Epoch 48/200\n",
      "648/648 [==============================] - 0s 681us/step - loss: 791022.1860 - val_loss: 1026625.6736\n",
      "Epoch 49/200\n",
      "648/648 [==============================] - 0s 668us/step - loss: 814915.7130 - val_loss: 964655.5347\n",
      "Epoch 50/200\n",
      "648/648 [==============================] - 0s 665us/step - loss: 806457.9352 - val_loss: 978224.3194\n",
      "Epoch 51/200\n",
      "648/648 [==============================] - 0s 757us/step - loss: 822389.8657 - val_loss: 956088.1111\n",
      "Epoch 52/200\n",
      "648/648 [==============================] - 0s 755us/step - loss: 795912.3194 - val_loss: 957797.0000\n",
      "Epoch 53/200\n",
      "648/648 [==============================] - 0s 641us/step - loss: 805153.4491 - val_loss: 992293.3194\n",
      "Epoch 54/200\n",
      "648/648 [==============================] - 0s 574us/step - loss: 838554.9807 - val_loss: 985244.8333\n",
      "Epoch 55/200\n",
      "648/648 [==============================] - 0s 739us/step - loss: 834230.4691 - val_loss: 980107.3194\n",
      "Epoch 56/200\n",
      "648/648 [==============================] - 0s 743us/step - loss: 830116.8256 - val_loss: 1006506.6111\n",
      "Epoch 57/200\n",
      "648/648 [==============================] - 0s 697us/step - loss: 838684.3449 - val_loss: 1018057.0139\n",
      "Epoch 58/200\n",
      "648/648 [==============================] - 0s 597us/step - loss: 827752.7994 - val_loss: 1032155.7222\n",
      "Epoch 59/200\n",
      "648/648 [==============================] - 1s 790us/step - loss: 830365.5463 - val_loss: 978954.9444\n",
      "Epoch 60/200\n",
      "648/648 [==============================] - 0s 673us/step - loss: 808818.5872 - val_loss: 1026297.4653\n",
      "Epoch 61/200\n",
      "648/648 [==============================] - 0s 625us/step - loss: 792481.5247 - val_loss: 1005456.5694\n",
      "Epoch 62/200\n",
      "648/648 [==============================] - 0s 682us/step - loss: 783968.3040 - val_loss: 945528.4514\n",
      "Epoch 63/200\n",
      "648/648 [==============================] - 0s 662us/step - loss: 818022.2847 - val_loss: 974464.5139\n",
      "Epoch 64/200\n",
      "648/648 [==============================] - 0s 631us/step - loss: 800105.3503 - val_loss: 971918.2778\n",
      "Epoch 65/200\n",
      "648/648 [==============================] - 0s 697us/step - loss: 828616.7068 - val_loss: 999556.1944\n",
      "Epoch 66/200\n",
      "648/648 [==============================] - 0s 677us/step - loss: 827323.3164 - val_loss: 958387.2083\n",
      "Epoch 67/200\n",
      "648/648 [==============================] - 0s 655us/step - loss: 775660.0517 - val_loss: 961273.5278\n",
      "Epoch 68/200\n",
      "648/648 [==============================] - 0s 659us/step - loss: 775559.5046 - val_loss: 972655.3750\n",
      "Epoch 69/200\n",
      "648/648 [==============================] - 0s 677us/step - loss: 782577.3951 - val_loss: 1443946.1250\n",
      "Epoch 70/200\n",
      "648/648 [==============================] - 0s 720us/step - loss: 791166.2986 - val_loss: 1126067.9583\n",
      "Epoch 71/200\n",
      "648/648 [==============================] - 0s 631us/step - loss: 806786.4028 - val_loss: 1011745.6181\n",
      "Epoch 72/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "648/648 [==============================] - 0s 626us/step - loss: 797437.8596 - val_loss: 1008885.4306\n",
      "Epoch 73/200\n",
      "648/648 [==============================] - 0s 594us/step - loss: 798310.2346 - val_loss: 997916.8125\n",
      "Epoch 74/200\n",
      "648/648 [==============================] - 0s 566us/step - loss: 820256.0239 - val_loss: 1036462.1076\n",
      "Epoch 75/200\n",
      "648/648 [==============================] - 0s 606us/step - loss: 778696.4769 - val_loss: 953519.1458\n",
      "Epoch 76/200\n",
      "648/648 [==============================] - 0s 700us/step - loss: 770502.5671 - val_loss: 997762.1667\n",
      "Epoch 77/200\n",
      "648/648 [==============================] - 0s 655us/step - loss: 776683.7562 - val_loss: 1044514.9653\n",
      "Epoch 78/200\n",
      "648/648 [==============================] - 0s 655us/step - loss: 783621.4946 - val_loss: 972762.6528\n",
      "Epoch 79/200\n",
      "648/648 [==============================] - 0s 639us/step - loss: 789180.0316 - val_loss: 1004047.2292\n",
      "Epoch 80/200\n",
      "648/648 [==============================] - 0s 658us/step - loss: 790765.7168 - val_loss: 1003909.0000\n",
      "Epoch 81/200\n",
      "648/648 [==============================] - 0s 670us/step - loss: 781043.4444 - val_loss: 969178.9653\n",
      "Epoch 82/200\n",
      "648/648 [==============================] - 0s 666us/step - loss: 762073.8056 - val_loss: 977065.1111\n",
      "Epoch 83/200\n",
      "648/648 [==============================] - 0s 654us/step - loss: 791237.4591 - val_loss: 977892.5139\n",
      "Epoch 84/200\n",
      "648/648 [==============================] - 0s 663us/step - loss: 805149.0154 - val_loss: 1074178.9931\n",
      "Epoch 85/200\n",
      "648/648 [==============================] - 0s 650us/step - loss: 815194.2608 - val_loss: 1016831.2917\n",
      "Epoch 86/200\n",
      "648/648 [==============================] - 0s 677us/step - loss: 816731.6049 - val_loss: 1052461.2222\n",
      "Epoch 87/200\n",
      "648/648 [==============================] - 0s 653us/step - loss: 816534.1273 - val_loss: 1061501.3125\n",
      "Epoch 88/200\n",
      "648/648 [==============================] - 0s 692us/step - loss: 822626.3796 - val_loss: 958501.5972\n",
      "Epoch 89/200\n",
      "648/648 [==============================] - 0s 655us/step - loss: 777989.4313 - val_loss: 957643.9028\n",
      "Epoch 90/200\n",
      "648/648 [==============================] - 0s 673us/step - loss: 757433.5046 - val_loss: 934427.1181\n",
      "Epoch 91/200\n",
      "648/648 [==============================] - 0s 674us/step - loss: 762757.3958 - val_loss: 945813.9028\n",
      "Epoch 92/200\n",
      "648/648 [==============================] - 0s 669us/step - loss: 792513.0478 - val_loss: 1024000.8403\n",
      "Epoch 93/200\n",
      "648/648 [==============================] - 0s 665us/step - loss: 760898.7832 - val_loss: 934299.9028\n",
      "Epoch 94/200\n",
      "648/648 [==============================] - 0s 691us/step - loss: 792397.7747 - val_loss: 967963.1806\n",
      "Epoch 95/200\n",
      "648/648 [==============================] - 0s 671us/step - loss: 794422.8364 - val_loss: 925816.9583\n",
      "Epoch 96/200\n",
      "648/648 [==============================] - 0s 722us/step - loss: 801707.8071 - val_loss: 975952.0833\n",
      "Epoch 97/200\n",
      "648/648 [==============================] - 0s 724us/step - loss: 791005.2176 - val_loss: 987687.7222\n",
      "Epoch 98/200\n",
      "648/648 [==============================] - 0s 767us/step - loss: 787316.3889 - val_loss: 1009212.9444\n",
      "Epoch 99/200\n",
      "648/648 [==============================] - 0s 718us/step - loss: 787209.8395 - val_loss: 986522.1111\n",
      "Epoch 100/200\n",
      "648/648 [==============================] - 0s 662us/step - loss: 784019.6651 - val_loss: 1013522.1806\n",
      "Epoch 101/200\n",
      "648/648 [==============================] - 0s 667us/step - loss: 797609.2647 - val_loss: 1007762.0000\n",
      "Epoch 102/200\n",
      "648/648 [==============================] - 0s 668us/step - loss: 814316.0509 - val_loss: 1000262.1042\n",
      "Epoch 103/200\n",
      "648/648 [==============================] - 0s 723us/step - loss: 808375.1914 - val_loss: 973284.7083\n",
      "Epoch 104/200\n",
      "648/648 [==============================] - 0s 692us/step - loss: 813274.5185 - val_loss: 1032625.5278\n",
      "Epoch 105/200\n",
      "648/648 [==============================] - 1s 786us/step - loss: 810683.5008 - val_loss: 1016279.4444\n",
      "Epoch 106/200\n",
      "648/648 [==============================] - 0s 574us/step - loss: 813046.3634 - val_loss: 949577.4861\n",
      "Epoch 107/200\n",
      "648/648 [==============================] - 0s 708us/step - loss: 811420.9522 - val_loss: 1021985.3403\n",
      "Epoch 108/200\n",
      "648/648 [==============================] - 0s 572us/step - loss: 817181.9668 - val_loss: 1006966.1389\n",
      "Epoch 109/200\n",
      "648/648 [==============================] - 0s 751us/step - loss: 808909.9568 - val_loss: 1003068.3264\n",
      "Epoch 110/200\n",
      "648/648 [==============================] - 0s 566us/step - loss: 808582.5216 - val_loss: 1051750.4722\n",
      "Epoch 111/200\n",
      "648/648 [==============================] - 0s 573us/step - loss: 815618.3426 - val_loss: 1027813.8889\n",
      "Epoch 112/200\n",
      "648/648 [==============================] - 0s 596us/step - loss: 814120.0054 - val_loss: 1057653.3333\n",
      "Epoch 113/200\n",
      "648/648 [==============================] - 1s 804us/step - loss: 803713.9722 - val_loss: 1010612.1389\n",
      "Epoch 114/200\n",
      "648/648 [==============================] - 1s 794us/step - loss: 811379.3619 - val_loss: 1064662.2222\n",
      "Epoch 115/200\n",
      "648/648 [==============================] - 1s 787us/step - loss: 808134.1373 - val_loss: 1023148.9236\n",
      "Epoch 116/200\n",
      "648/648 [==============================] - 0s 763us/step - loss: 807720.2114 - val_loss: 1046064.5833\n",
      "Epoch 117/200\n",
      "648/648 [==============================] - 0s 655us/step - loss: 807617.3866 - val_loss: 1040594.8194\n",
      "Epoch 118/200\n",
      "648/648 [==============================] - 0s 751us/step - loss: 816815.0370 - val_loss: 1054766.9931\n",
      "Epoch 119/200\n",
      "648/648 [==============================] - 0s 736us/step - loss: 813629.2654 - val_loss: 1035418.9028\n",
      "Epoch 120/200\n",
      "648/648 [==============================] - 0s 651us/step - loss: 802507.4460 - val_loss: 1030327.5278\n",
      "Epoch 121/200\n",
      "648/648 [==============================] - 0s 708us/step - loss: 802288.4730 - val_loss: 981443.2153\n",
      "Epoch 122/200\n",
      "648/648 [==============================] - 0s 621us/step - loss: 802580.1289 - val_loss: 975811.2917\n",
      "Epoch 123/200\n",
      "648/648 [==============================] - 0s 633us/step - loss: 804115.4035 - val_loss: 990300.9722\n",
      "Epoch 124/200\n",
      "648/648 [==============================] - 0s 700us/step - loss: 804831.9321 - val_loss: 981477.0972\n",
      "Epoch 125/200\n",
      "648/648 [==============================] - 0s 699us/step - loss: 802017.3765 - val_loss: 987376.2500\n",
      "Epoch 126/200\n",
      "648/648 [==============================] - 0s 725us/step - loss: 810090.8125 - val_loss: 1034254.3194\n",
      "Epoch 127/200\n",
      "648/648 [==============================] - 0s 625us/step - loss: 805945.5610 - val_loss: 991052.2639\n",
      "Epoch 128/200\n",
      "648/648 [==============================] - 0s 639us/step - loss: 814704.0664 - val_loss: 1022445.7778\n",
      "Epoch 129/200\n",
      "648/648 [==============================] - 0s 694us/step - loss: 815281.5995 - val_loss: 1044331.2778\n",
      "Epoch 130/200\n",
      "648/648 [==============================] - 0s 715us/step - loss: 811086.7693 - val_loss: 1051303.3194\n",
      "Epoch 131/200\n",
      "648/648 [==============================] - 0s 694us/step - loss: 806961.2346 - val_loss: 1019275.6597\n",
      "Epoch 132/200\n",
      "648/648 [==============================] - 0s 690us/step - loss: 816702.8210 - val_loss: 1042419.7083\n",
      "Epoch 133/200\n",
      "648/648 [==============================] - 0s 731us/step - loss: 812613.6559 - val_loss: 1027525.6528\n",
      "Epoch 134/200\n",
      "648/648 [==============================] - 1s 784us/step - loss: 804829.4691 - val_loss: 970443.5903\n",
      "Epoch 135/200\n",
      "648/648 [==============================] - 0s 676us/step - loss: 806646.8233 - val_loss: 986844.4792\n",
      "Epoch 136/200\n",
      "648/648 [==============================] - 0s 659us/step - loss: 809340.2438 - val_loss: 989427.3125\n",
      "Epoch 137/200\n",
      "648/648 [==============================] - 0s 652us/step - loss: 808952.0471 - val_loss: 947088.1250\n",
      "Epoch 138/200\n",
      "648/648 [==============================] - 0s 655us/step - loss: 799128.5602 - val_loss: 971400.9236\n",
      "Epoch 139/200\n",
      "648/648 [==============================] - 0s 658us/step - loss: 803435.5725 - val_loss: 960687.1667\n",
      "Epoch 140/200\n",
      "648/648 [==============================] - 0s 656us/step - loss: 811756.0216 - val_loss: 1024346.7292\n",
      "Epoch 141/200\n",
      "648/648 [==============================] - 0s 671us/step - loss: 805922.0525 - val_loss: 1018943.0000\n",
      "Epoch 142/200\n",
      "648/648 [==============================] - 0s 670us/step - loss: 803434.5965 - val_loss: 1037925.3681\n",
      "Epoch 143/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "648/648 [==============================] - 0s 602us/step - loss: 806522.5370 - val_loss: 1025172.0694\n",
      "Epoch 144/200\n",
      "648/648 [==============================] - 0s 659us/step - loss: 805933.6003 - val_loss: 963467.0278\n",
      "Epoch 145/200\n",
      "648/648 [==============================] - 0s 666us/step - loss: 800659.1921 - val_loss: 924504.5833\n",
      "Epoch 146/200\n",
      "648/648 [==============================] - 0s 566us/step - loss: 796252.9066 - val_loss: 979126.2569\n",
      "Epoch 147/200\n",
      "648/648 [==============================] - 0s 568us/step - loss: 788425.4360 - val_loss: 960551.8507\n",
      "Epoch 148/200\n",
      "648/648 [==============================] - 0s 728us/step - loss: 757378.5795 - val_loss: 924986.6806\n",
      "Epoch 149/200\n",
      "648/648 [==============================] - 0s 700us/step - loss: 746980.6049 - val_loss: 930937.1944\n",
      "Epoch 150/200\n",
      "648/648 [==============================] - 0s 675us/step - loss: 749877.0123 - val_loss: 926697.1181\n",
      "Epoch 151/200\n",
      "648/648 [==============================] - 0s 670us/step - loss: 746316.1983 - val_loss: 918452.8507\n",
      "Epoch 152/200\n",
      "648/648 [==============================] - 0s 690us/step - loss: 761324.6188 - val_loss: 914064.2847\n",
      "Epoch 153/200\n",
      "648/648 [==============================] - 0s 657us/step - loss: 752770.7577 - val_loss: 932362.0278\n",
      "Epoch 154/200\n",
      "648/648 [==============================] - 0s 663us/step - loss: 747440.6698 - val_loss: 949991.9167\n",
      "Epoch 155/200\n",
      "648/648 [==============================] - 0s 647us/step - loss: 749998.4985 - val_loss: 955084.8194\n",
      "Epoch 156/200\n",
      "648/648 [==============================] - 0s 681us/step - loss: 772985.9753 - val_loss: 1034120.4861\n",
      "Epoch 157/200\n",
      "648/648 [==============================] - 0s 693us/step - loss: 808291.0802 - val_loss: 910497.5278\n",
      "Epoch 158/200\n",
      "648/648 [==============================] - 0s 720us/step - loss: 756077.4938 - val_loss: 912648.8090\n",
      "Epoch 159/200\n",
      "648/648 [==============================] - 0s 687us/step - loss: 748480.5586 - val_loss: 925400.1042\n",
      "Epoch 160/200\n",
      "648/648 [==============================] - 0s 663us/step - loss: 763190.8742 - val_loss: 958559.2361\n",
      "Epoch 161/200\n",
      "648/648 [==============================] - 0s 653us/step - loss: 752674.5941 - val_loss: 916847.9653\n",
      "Epoch 162/200\n",
      "648/648 [==============================] - 0s 675us/step - loss: 766599.2407 - val_loss: 921967.1250\n",
      "Epoch 163/200\n",
      "648/648 [==============================] - 0s 717us/step - loss: 741711.0432 - val_loss: 931086.3264\n",
      "Epoch 164/200\n",
      "648/648 [==============================] - 0s 680us/step - loss: 751575.6898 - val_loss: 917408.6667\n",
      "Epoch 165/200\n",
      "648/648 [==============================] - 0s 675us/step - loss: 774425.3889 - val_loss: 1033908.6250\n",
      "Epoch 166/200\n",
      "648/648 [==============================] - 0s 687us/step - loss: 817268.5826 - val_loss: 1003776.0486\n",
      "Epoch 167/200\n",
      "648/648 [==============================] - 0s 679us/step - loss: 813766.8179 - val_loss: 990216.2708\n",
      "Epoch 168/200\n",
      "648/648 [==============================] - 0s 699us/step - loss: 805732.8951 - val_loss: 962435.3750\n",
      "Epoch 169/200\n",
      "648/648 [==============================] - 0s 643us/step - loss: 801945.8349 - val_loss: 962518.8542\n",
      "Epoch 170/200\n",
      "648/648 [==============================] - 0s 681us/step - loss: 807550.8858 - val_loss: 1018177.3958\n",
      "Epoch 171/200\n",
      "648/648 [==============================] - 0s 673us/step - loss: 807601.6968 - val_loss: 1007124.9861\n",
      "Epoch 172/200\n",
      "648/648 [==============================] - 0s 645us/step - loss: 799139.9915 - val_loss: 974151.4931\n",
      "Epoch 173/200\n",
      "648/648 [==============================] - 0s 661us/step - loss: 808348.8750 - val_loss: 1012896.5486\n",
      "Epoch 174/200\n",
      "648/648 [==============================] - 0s 693us/step - loss: 798469.5262 - val_loss: 1007304.2292\n",
      "Epoch 175/200\n",
      "648/648 [==============================] - 0s 693us/step - loss: 800451.1019 - val_loss: 956601.2569\n",
      "Epoch 176/200\n",
      "648/648 [==============================] - 0s 666us/step - loss: 804315.0401 - val_loss: 979003.0000\n",
      "Epoch 177/200\n",
      "648/648 [==============================] - 0s 659us/step - loss: 806524.8688 - val_loss: 957813.3819\n",
      "Epoch 178/200\n",
      "648/648 [==============================] - 0s 700us/step - loss: 802269.5880 - val_loss: 958166.1667\n",
      "Epoch 179/200\n",
      "648/648 [==============================] - 0s 682us/step - loss: 795103.7423 - val_loss: 971975.8056\n",
      "Epoch 180/200\n",
      "648/648 [==============================] - 0s 640us/step - loss: 797950.2423 - val_loss: 998667.3750\n",
      "Epoch 181/200\n",
      "648/648 [==============================] - 0s 650us/step - loss: 804037.9606 - val_loss: 979575.5625\n",
      "Epoch 182/200\n",
      "648/648 [==============================] - 0s 667us/step - loss: 800092.1219 - val_loss: 975976.8819\n",
      "Epoch 183/200\n",
      "648/648 [==============================] - 0s 669us/step - loss: 789372.8974 - val_loss: 908130.2639\n",
      "Epoch 184/200\n",
      "648/648 [==============================] - 0s 670us/step - loss: 756191.1366 - val_loss: 955759.6424\n",
      "Epoch 185/200\n",
      "648/648 [==============================] - 0s 664us/step - loss: 771105.4267 - val_loss: 936437.2639\n",
      "Epoch 186/200\n",
      "648/648 [==============================] - 0s 676us/step - loss: 756084.4907 - val_loss: 919248.6319\n",
      "Epoch 187/200\n",
      "648/648 [==============================] - 0s 669us/step - loss: 748889.9159 - val_loss: 920127.8889\n",
      "Epoch 188/200\n",
      "648/648 [==============================] - 0s 686us/step - loss: 772532.6157 - val_loss: 924277.2569\n",
      "Epoch 189/200\n",
      "648/648 [==============================] - 0s 655us/step - loss: 752838.5247 - val_loss: 912377.9410\n",
      "Epoch 190/200\n",
      "648/648 [==============================] - 0s 664us/step - loss: 748014.1057 - val_loss: 910185.2257\n",
      "Epoch 191/200\n",
      "648/648 [==============================] - 0s 653us/step - loss: 740945.9599 - val_loss: 920027.7431\n",
      "Epoch 192/200\n",
      "648/648 [==============================] - 0s 663us/step - loss: 746755.6605 - val_loss: 920266.1701\n",
      "Epoch 193/200\n",
      "648/648 [==============================] - 0s 661us/step - loss: 741357.1219 - val_loss: 923560.0035\n",
      "Epoch 194/200\n",
      "648/648 [==============================] - 0s 666us/step - loss: 736119.3441 - val_loss: 902904.1528\n",
      "Epoch 195/200\n",
      "648/648 [==============================] - 0s 675us/step - loss: 746497.7878 - val_loss: 931178.1111\n",
      "Epoch 196/200\n",
      "648/648 [==============================] - 0s 680us/step - loss: 745783.0725 - val_loss: 926179.7431\n",
      "Epoch 197/200\n",
      "648/648 [==============================] - 0s 648us/step - loss: 751172.1420 - val_loss: 964365.2535\n",
      "Epoch 198/200\n",
      "648/648 [==============================] - 0s 737us/step - loss: 759736.3503 - val_loss: 942906.5000\n",
      "Epoch 199/200\n",
      "648/648 [==============================] - 0s 641us/step - loss: 752089.8819 - val_loss: 917511.7847\n",
      "Epoch 200/200\n",
      "648/648 [==============================] - 0s 754us/step - loss: 745905.6505 - val_loss: 902187.2674\n"
     ]
    }
   ],
   "source": [
    "#Fit the model to data\n",
    "history = model.fit(x = Input, y= y, batch_size= 32, epochs = 200, verbose = 1, validation_split = 0.1, shuffle = True  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c080dd41b69a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0ml_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0ml_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0ml_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "l_test = (history.history['val_loss'])\n",
    "l_train = (history.history['loss'])\n",
    "l_test = np.sqrt(history.history['val_loss'])\n",
    "l_train = np.sqrt(history.history['loss'])\n",
    "plt.plot(l_train, label=\"Training Loss\")\n",
    "plt.plot(l_test, label=\"Test Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 995.0578   784.95935]\n",
      " [ 747.85504  601.9293 ]\n",
      " [ 995.0578   784.95935]\n",
      " [ 995.0578   784.95935]\n",
      " [ 904.98224  767.04724]\n",
      " [1580.4026   725.70636]\n",
      " [ 265.7959  -737.6951 ]\n",
      " [ 934.54315  793.724  ]\n",
      " [ 995.0578   784.95935]\n",
      " [ 995.0578   784.95935]]\n",
      "[[2197.89312   -238.09536  ]\n",
      " [ 511.68      1078.24752  ]\n",
      " [ 142.275072  1302.79032  ]\n",
      " [ 173.42272     16.9751448]\n",
      " [  68.96832     16.1958168]\n",
      " [1630.16192   -468.62352  ]\n",
      " [ 271.971328  -751.5576   ]\n",
      " [ 718.98112   1027.07064  ]\n",
      " [ 173.488896  1280.47824  ]\n",
      " [ 136.454016  1321.77888  ]]\n"
     ]
    }
   ],
   "source": [
    "#model.outputs[0]\n",
    "#m.outputs = \n",
    "y_pred = model.predict(Input)\n",
    "print(y_pred[0:10])\n",
    "print(y[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([  0.,   0.,   0.,   0.,   1.,   2.,  10., 296.,   5., 406.]),\n",
       "  array([  1.,   0.,   0.,   2.,   3.,   1., 713.,   0.,   0.,   0.])],\n",
       " array([-737.6951  , -499.05222 , -260.40927 ,  -21.766352,  216.87657 ,\n",
       "         455.5195  ,  694.1624  ,  932.80536 , 1171.4482  , 1410.0912  ,\n",
       "        1648.7341  ], dtype=float32),\n",
       " <a list of 2 Lists of Patches objects>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([  3.,  64., 202., 106., 104., 236.,   3.,   1.,   0.,   1.]),\n",
       "  array([  5.,  74., 281., 356.,   4.,   0.,   0.,   0.,   0.,   0.])],\n",
       " array([-983.20512 , -393.042048,  197.121024,  787.284096, 1377.447168,\n",
       "        1967.61024 , 2557.773312, 3147.936384, 3738.099456, 4328.262528,\n",
       "        4918.4256  ]),\n",
       " <a list of 2 Lists of Patches objects>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[950.71747 817.4354 ]\n",
      "  [956.105   816.6341 ]\n",
      "  [949.5693  813.35016]\n",
      "  [937.4784  813.0262 ]\n",
      "  [911.65314 805.8622 ]\n",
      "  [871.5379  816.5585 ]\n",
      "  [      nan       nan]]]\n",
      "[[[950.71744 817.43544]\n",
      "  [956.10496 816.63408]\n",
      "  [949.56928 813.35016]\n",
      "  [937.4784  813.02616]\n",
      "  [911.65312 805.86216]\n",
      "  [871.53792 816.55848]\n",
      "  [      inf       inf]]]\n"
     ]
    }
   ],
   "source": [
    "layer_name = 'my_layer'\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer('mask_layer').output)\n",
    "intermediate_output = intermediate_layer_model.predict(Input[107:108])\n",
    "print(intermediate_output)\n",
    "print(Input[107:108])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.Masking at 0x7f292a90ff28>"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer('masking_30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 6, 2)"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Input[0:1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': [1579796.705882353,\n",
       "  1579553.3946078431,\n",
       "  1579275.7083333333,\n",
       "  1579000.8848039217,\n",
       "  1578702.931372549,\n",
       "  1578393.8995098039,\n",
       "  1578038.7745098039,\n",
       "  1577697.012254902,\n",
       "  1577373.3406862745,\n",
       "  1577030.9534313725,\n",
       "  1576676.9411764706,\n",
       "  1576311.6715686275,\n",
       "  1575965.4632352942,\n",
       "  1575604.5637254901,\n",
       "  1575267.1348039217,\n",
       "  1574929.8284313725,\n",
       "  1574596.4093137255,\n",
       "  1574264.6102941176,\n",
       "  1573941.7426470588,\n",
       "  1573619.9460784313,\n",
       "  1573299.9142156863,\n",
       "  1572995.556372549,\n",
       "  1572689.5906862745,\n",
       "  1572395.9044117648,\n",
       "  1572099.556372549,\n",
       "  1571808.0710784313,\n",
       "  1571516.3578431373,\n",
       "  1571232.9117647058,\n",
       "  1570951.2916666667,\n",
       "  1570691.6838235294,\n",
       "  1570431.7034313725,\n",
       "  1570180.7303921569,\n",
       "  1569928.056372549,\n",
       "  1569678.4289215687,\n",
       "  1569440.3357843137,\n",
       "  1569210.5196078431,\n",
       "  1568982.281862745,\n",
       "  1568765.669117647,\n",
       "  1568553.7450980393,\n",
       "  1568357.7401960783,\n",
       "  1568153.9485294118,\n",
       "  1567946.2132352942,\n",
       "  1567759.4852941176,\n",
       "  1567569.693627451,\n",
       "  1567395.2401960783,\n",
       "  1567238.4166666667,\n",
       "  1567087.9926470588,\n",
       "  1566941.1446078431,\n",
       "  1566800.6102941176,\n",
       "  1566665.762254902,\n",
       "  1566534.075980392,\n",
       "  1566405.2720588236,\n",
       "  1566276.6495098039,\n",
       "  1566148.4215686275,\n",
       "  1566031.7671568627,\n",
       "  1565909.362745098,\n",
       "  1565793.8235294118,\n",
       "  1565678.3137254901,\n",
       "  1565565.794117647,\n",
       "  1565452.9950980393,\n",
       "  1565342.843137255,\n",
       "  1565229.7083333333,\n",
       "  1565117.5049019607,\n",
       "  1565005.6323529412,\n",
       "  1564893.6029411764,\n",
       "  1564781.468137255,\n",
       "  1564672.8970588236,\n",
       "  1564560.2745098039,\n",
       "  1564453.1151960783,\n",
       "  1564343.487745098,\n",
       "  1564235.9019607843,\n",
       "  1564126.6470588236,\n",
       "  1564022.056372549,\n",
       "  1563917.4975490195,\n",
       "  1563808.012254902,\n",
       "  1563702.806372549,\n",
       "  1563593.8504901961,\n",
       "  1563488.3823529412,\n",
       "  1563379.612745098,\n",
       "  1563271.5343137255,\n",
       "  1563162.1299019607,\n",
       "  1563054.8970588236,\n",
       "  1562952.6985294118,\n",
       "  1562846.450980392,\n",
       "  1562745.205882353,\n",
       "  1562639.4117647058,\n",
       "  1562540.5269607843,\n",
       "  1562436.5294117648,\n",
       "  1562332.7034313725,\n",
       "  1562229.487745098,\n",
       "  1562123.7843137255,\n",
       "  1562022.806372549,\n",
       "  1561919.4362745099,\n",
       "  1561813.9705882352,\n",
       "  1561714.7573529412,\n",
       "  1561613.262254902,\n",
       "  1561505.5147058824,\n",
       "  1561407.3946078431,\n",
       "  1561304.0367647058,\n",
       "  1561202.044117647,\n",
       "  1561101.6274509805,\n",
       "  1560998.7720588236,\n",
       "  1560894.306372549,\n",
       "  1560791.9338235294,\n",
       "  1560687.625,\n",
       "  1560586.7573529412,\n",
       "  1560482.9779411764,\n",
       "  1560377.9803921569,\n",
       "  1560276.2328431373,\n",
       "  1560174.3333333333,\n",
       "  1560073.2450980393,\n",
       "  1559971.1544117648,\n",
       "  1559865.468137255,\n",
       "  1559763.943627451,\n",
       "  1559659.4607843137,\n",
       "  1559555.919117647,\n",
       "  1559452.330882353,\n",
       "  1559351.7107843137,\n",
       "  1559245.8995098039,\n",
       "  1559146.218137255,\n",
       "  1559042.906862745,\n",
       "  1558941.318627451,\n",
       "  1558837.4950980393,\n",
       "  1558735.294117647,\n",
       "  1558633.25,\n",
       "  1558529.799019608,\n",
       "  1558427.0539215687,\n",
       "  1558330.075980392,\n",
       "  1558227.7230392157,\n",
       "  1558121.7401960783,\n",
       "  1558020.1348039217,\n",
       "  1557915.4803921569,\n",
       "  1557816.424019608,\n",
       "  1557712.2426470588,\n",
       "  1557611.2769607843,\n",
       "  1557507.093137255,\n",
       "  1557403.2965686275,\n",
       "  1557300.1274509805,\n",
       "  1557201.7034313725,\n",
       "  1557101.237745098,\n",
       "  1556996.806372549,\n",
       "  1556892.7450980393,\n",
       "  1556790.700980392,\n",
       "  1556684.9534313725,\n",
       "  1556584.943627451,\n",
       "  1556484.7892156863,\n",
       "  1556380.0588235294,\n",
       "  1556279.799019608,\n",
       "  1556177.5367647058,\n",
       "  1556072.318627451,\n",
       "  1555972.137254902,\n",
       "  1555867.8578431373,\n",
       "  1555769.2573529412,\n",
       "  1555663.4166666667,\n",
       "  1555561.4044117648,\n",
       "  1555454.2647058824,\n",
       "  1555354.8014705882,\n",
       "  1555255.9632352942,\n",
       "  1555149.7916666667,\n",
       "  1555050.455882353,\n",
       "  1554947.4926470588,\n",
       "  1554843.9926470588,\n",
       "  1554743.419117647,\n",
       "  1554641.6299019607,\n",
       "  1554541.2745098039,\n",
       "  1554437.0269607843,\n",
       "  1554334.2647058824,\n",
       "  1554232.7205882352,\n",
       "  1554127.0049019607,\n",
       "  1554026.1348039217,\n",
       "  1553923.0,\n",
       "  1553821.6887254901,\n",
       "  1553720.193627451,\n",
       "  1553616.5882352942,\n",
       "  1553515.281862745,\n",
       "  1553410.8799019607,\n",
       "  1553308.8039215687,\n",
       "  1553204.5710784313,\n",
       "  1553103.718137255,\n",
       "  1553000.1299019607,\n",
       "  1552898.1323529412,\n",
       "  1552798.9975490195,\n",
       "  1552695.2352941176,\n",
       "  1552594.0514705882,\n",
       "  1552489.3946078431,\n",
       "  1552388.887254902,\n",
       "  1552285.25,\n",
       "  1552182.0955882352,\n",
       "  1552084.1519607843,\n",
       "  1551980.2328431373,\n",
       "  1551877.299019608,\n",
       "  1551774.9754901961,\n",
       "  1551672.8553921569,\n",
       "  1551569.468137255,\n",
       "  1551468.049019608,\n",
       "  1551366.693627451,\n",
       "  1551264.6495098039,\n",
       "  1551164.2965686275,\n",
       "  1551059.1593137255,\n",
       "  1550955.8161764706],\n",
       " 'loss': [1496424.6592920355,\n",
       "  1496221.8938053097,\n",
       "  1496001.039823009,\n",
       "  1495771.549778761,\n",
       "  1495526.3838495575,\n",
       "  1495260.8550884956,\n",
       "  1494993.6946902655,\n",
       "  1494739.9845132744,\n",
       "  1494458.860619469,\n",
       "  1494127.2345132744,\n",
       "  1493850.6836283186,\n",
       "  1493518.674778761,\n",
       "  1493220.6305309734,\n",
       "  1492872.9867256638,\n",
       "  1492573.435840708,\n",
       "  1492230.7754424778,\n",
       "  1491945.1305309734,\n",
       "  1491585.3219026548,\n",
       "  1491301.5475663717,\n",
       "  1491020.2588495575,\n",
       "  1490642.0553097345,\n",
       "  1490455.1659292036,\n",
       "  1490101.2522123894,\n",
       "  1489835.9192477877,\n",
       "  1489580.4485619469,\n",
       "  1489288.7876106196,\n",
       "  1489035.274336283,\n",
       "  1488805.5719026548,\n",
       "  1488485.7334070797,\n",
       "  1488285.5807522123,\n",
       "  1487966.7212389382,\n",
       "  1487806.3384955751,\n",
       "  1487499.9480088495,\n",
       "  1487344.0088495575,\n",
       "  1487083.9336283186,\n",
       "  1486861.710176991,\n",
       "  1486703.610619469,\n",
       "  1486541.4723451328,\n",
       "  1486285.2887168142,\n",
       "  1486091.6913716814,\n",
       "  1485838.703539823,\n",
       "  1485789.5707964601,\n",
       "  1485606.185840708,\n",
       "  1485477.185840708,\n",
       "  1485199.3407079645,\n",
       "  1484981.0044247787,\n",
       "  1484965.503318584,\n",
       "  1484793.9446902655,\n",
       "  1484654.982300885,\n",
       "  1484491.5730088495,\n",
       "  1484416.7157079645,\n",
       "  1484253.2223451328,\n",
       "  1484239.914823009,\n",
       "  1483967.6814159292,\n",
       "  1484006.4363938053,\n",
       "  1483822.646017699,\n",
       "  1483769.2024336283,\n",
       "  1483648.1526548672,\n",
       "  1483540.8550884956,\n",
       "  1483412.6205752213,\n",
       "  1483376.0088495575,\n",
       "  1483201.8185840708,\n",
       "  1483102.6946902655,\n",
       "  1483005.0342920355,\n",
       "  1482895.046460177,\n",
       "  1482854.0298672565,\n",
       "  1482775.6211283186,\n",
       "  1482615.0265486725,\n",
       "  1482557.9646017698,\n",
       "  1482310.9026548672,\n",
       "  1482260.4225663717,\n",
       "  1482127.7942477877,\n",
       "  1482177.5984513275,\n",
       "  1482038.6338495575,\n",
       "  1481930.9712389382,\n",
       "  1481847.065818584,\n",
       "  1481684.649336283,\n",
       "  1481711.5154867256,\n",
       "  1481599.0862831858,\n",
       "  1481463.7146017698,\n",
       "  1481467.9745575222,\n",
       "  1481231.3163716814,\n",
       "  1481109.3207964601,\n",
       "  1480987.3694690266,\n",
       "  1481021.1183628319,\n",
       "  1480778.9048672565,\n",
       "  1480763.0212942478,\n",
       "  1480820.6128318585,\n",
       "  1480602.0918141592,\n",
       "  1480559.5995575222,\n",
       "  1480378.9778761063,\n",
       "  1480250.246681416,\n",
       "  1480194.0840707964,\n",
       "  1480060.1703539824,\n",
       "  1479985.6670353983,\n",
       "  1479971.9292035399,\n",
       "  1479817.7986725664,\n",
       "  1479729.6139380531,\n",
       "  1479629.1349557522,\n",
       "  1479520.9059734512,\n",
       "  1479447.2787610618,\n",
       "  1479372.5995575222,\n",
       "  1479288.9524336283,\n",
       "  1479187.6570796461,\n",
       "  1479103.435840708,\n",
       "  1479017.664823009,\n",
       "  1478937.7190265488,\n",
       "  1478856.2522123894,\n",
       "  1478659.9026548672,\n",
       "  1478599.3451327435,\n",
       "  1478528.6996681415,\n",
       "  1478528.6338495575,\n",
       "  1478328.439159292,\n",
       "  1478254.8727876106,\n",
       "  1478218.75,\n",
       "  1478147.2787610618,\n",
       "  1477967.0331858408,\n",
       "  1477904.6886061947,\n",
       "  1477808.203539823,\n",
       "  1477748.831858407,\n",
       "  1477641.3816371681,\n",
       "  1477373.0276548672,\n",
       "  1477260.8384955751,\n",
       "  1477339.671460177,\n",
       "  1477300.4646017698,\n",
       "  1477129.7345132744,\n",
       "  1477010.692477876,\n",
       "  1477049.9314159292,\n",
       "  1476878.041482301,\n",
       "  1476650.0199115044,\n",
       "  1476725.4115044249,\n",
       "  1476607.6592920355,\n",
       "  1476421.378318584,\n",
       "  1476436.1482300884,\n",
       "  1476338.923119469,\n",
       "  1476321.0066371681,\n",
       "  1476093.8053097345,\n",
       "  1476030.9026548672,\n",
       "  1476005.8727876106,\n",
       "  1475787.3866150442,\n",
       "  1475738.8274336283,\n",
       "  1475695.517699115,\n",
       "  1475589.0331858408,\n",
       "  1475438.9336283186,\n",
       "  1475399.5486725664,\n",
       "  1475234.4192477877,\n",
       "  1475316.8097345133,\n",
       "  1475094.8672566372,\n",
       "  1475161.6438053097,\n",
       "  1474950.8030973452,\n",
       "  1474817.762721239,\n",
       "  1474555.331858407,\n",
       "  1474635.5284845133,\n",
       "  1474565.314159292,\n",
       "  1474452.4867256638,\n",
       "  1474349.0022123894,\n",
       "  1474264.5475663717,\n",
       "  1474317.649336283,\n",
       "  1474036.8938053097,\n",
       "  1473896.1803097345,\n",
       "  1473943.168141593,\n",
       "  1473927.8628318585,\n",
       "  1473869.3794247787,\n",
       "  1473605.918141593,\n",
       "  1473518.8448561947,\n",
       "  1473555.581858407,\n",
       "  1473424.564159292,\n",
       "  1473270.7278761063,\n",
       "  1473277.185840708,\n",
       "  1472974.3871681415,\n",
       "  1473003.4004424778,\n",
       "  1472920.1725663717,\n",
       "  1472867.6991150442,\n",
       "  1472751.8451327435,\n",
       "  1472646.7212389382,\n",
       "  1472581.110619469,\n",
       "  1472508.7876106196,\n",
       "  1472442.2577433628,\n",
       "  1472383.6438053097,\n",
       "  1472096.3462389382,\n",
       "  1472049.639380531,\n",
       "  1472016.8097345133,\n",
       "  1471836.6504424778,\n",
       "  1471674.9646017698,\n",
       "  1471779.603982301,\n",
       "  1471492.0990044249,\n",
       "  1471416.621681416,\n",
       "  1471503.2654867256,\n",
       "  1471321.9903207964,\n",
       "  1471085.7887168142,\n",
       "  1471168.210176991,\n",
       "  1471044.7876106196,\n",
       "  1470816.335176991,\n",
       "  1470895.914823009,\n",
       "  1470684.4126106196,\n",
       "  1470738.4513274336,\n",
       "  1470868.1172566372,\n",
       "  1470525.3329646017,\n",
       "  1470508.0055309734,\n",
       "  1470260.8517699116]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4918.4256"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.4000e+02, 2.0941e+04, 4.4879e+04, 2.7261e+04, 5.3311e+04,\n",
       "        3.1400e+02, 1.8000e+01, 4.0000e+00, 0.0000e+00, 2.3800e+02]),\n",
       " array([-0.557279 , -0.2412341,  0.0748108,  0.3908557,  0.7069006,\n",
       "         1.0229455,  1.3389904,  1.6550353,  1.9710802,  2.2871251,\n",
       "         2.60317  ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "data_dir = \"/home/niteesh/Documents/uni/HCI/Saarland/Supplementary/DATA/GazeData\"\n",
    "files = os.listdir(data_dir)\n",
    "files.sort()\n",
    "\n",
    "files = [files[6]]\n",
    "for f in files:\n",
    "    file = data_dir + \"/\" + f\n",
    "    data = np.genfromtxt(file, skip_header=1, delimiter='|')\n",
    "\n",
    "plt.hist(data[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
